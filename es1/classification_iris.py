# -*- coding: utf-8 -*-
"""classification_iris.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I2qU4elWwbVAn2gEnSxcAFb5XMSvD_JQ

# # Classifiers introduction

In the following program we introduce the basic steps of classification of a dataset in a matrix

Import the package for learning and modeling trees
"""

from sklearn import tree

"""Define the matrix containing the data (one example per row)
and the vector containing the corresponding target value
"""

X = [[0, 0, 0], [1, 1, 1], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]]
Y = [1, 0, 0, 0, 1, 1]

"""Declare the classification model you want to use and then fit the model to the data"""

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

"""Predict the target value (and print it) for the passed data, using the fitted model currently in clf"""

print(clf.predict([[0, 1, 1]]))

print(clf.predict([[1, 0, 1],[0, 0, 1]]))

import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph.render(view=False)

"""In the following we start using a dataset (from UCI Machine Learning repository)"""

from sklearn.datasets import load_iris
iris = load_iris()

"""# Declare the type of prediction model and the working criteria for the model induction algorithm"""
"""min_sample_leaf minimo numero delle foglie"""
clf = tree.DecisionTreeClassifier(criterion="entropy",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})

"""# Split the dataset in training and test set"""

# Generate a random permutation of the indices of examples that will be later used
# for the training and the test set
import numpy as np
np.random.seed(0)
indices = np.random.permutation(len(iris.data))

# We now decide to keep the last 10 indices for test set, the remaining for the training set
indices_training=indices[:-10]
indices_test=indices[-10:]

iris_X_train_boosted = iris.data[indices_training] # keep for training all the matrix elements with the exception of the last 10
iris_y_train_boosted = iris.target[indices_training]
iris_X_test_boosted  = iris.data[indices_test] # keep the last 10 elements for test set
iris_y_test_boosted  = iris.target[indices_test]



"""# Fit the learning model on training set"""

# fit the model to the training data
clf = clf.fit(iris_X_train_boosted, iris_y_train_boosted)

"""# Obtain predictions"""

# apply fitted model "clf" to the test set
predicted_y_test = clf.predict(iris_X_test_boosted)

# print the predictions (class numbers associated to classes names in target names)
print("Predictions:")
print(predicted_y_test)
print("True classes:")
print(iris_y_test_boosted)
print(iris.target_names)

"""Print the index of the test instances and the corresponding predictions"""

# print the corresponding instances indexes and class names
for i in range(len(iris_y_test_boosted)):
    print("Instance # "+str(indices_test[i])+": ")
    print("Predicted: " + iris.target_names[predicted_y_test[i]] +"\t True: " + iris.target_names[iris_y_test_boosted[i]] + "\n")

"""# Look at the specific examples"""

for i in range(len(iris_y_test_boosted)):
    print("Instance # "+str(indices_test)+": ")
    s=""
    for j in range(len(iris.feature_names)):
        s=s+iris.feature_names[j]+"="+str(iris_X_test_boosted[i][j])
        if (j<len(iris.feature_names)-1): s=s+", "
    print(s)
    print("Predicted: " + iris.target_names[predicted_y_test[i]] +"\t True: " + iris.target_names[iris_y_test_boosted[i]] + "\n")

"""# Obtain model performance results"""

# print some metrics results
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
acc_score = accuracy_score(iris_y_test_boosted, predicted_y_test)
print("Accuracy score: "+ str(acc_score))
f1=f1_score(iris_y_test_boosted, predicted_y_test, average='macro')
print("F1 score: "+str(f1))

"""# Use Cross Validation"""

from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score # will be used to separate training and test
iris = load_iris()
clf = tree.DecisionTreeClassifier(criterion="entropy",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})
clf = clf.fit(iris.data, iris.target)
scores = cross_val_score(clf, iris.data, iris.target, cv=5) # score will be the accuracy
print(scores)

# computes F1- score
f1_scores = cross_val_score(clf, iris.data, iris.target, cv=5, scoring='f1_macro')
print(f1_scores)

"""# Show the resulting tree

## 1. Print the picture in a PDF file
"""

import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph.render("my_iris_predictions",view=False)

"""## 2. Generate a picture here"""

print(list(iris.feature_names))
print(list(iris.target_names))

dot_data = tree.export_graphviz(clf, out_file=None,
                         feature_names=iris.feature_names,
                         class_names=iris.target_names,
                         filled=True, rounded=True,
                         special_characters=True)
graph = graphviz.Source(dot_data)
graph.render(view=False)



"""# Your work: what you have to do
Modify the given Jupyter notebook on decision trees on Iris data and perform the following tasks:

1. get an artificial inflation of some class in the training set by a given factor: 10 (weigh more the classes virginica e versicolor which are more difficult to discriminate). Learn the tree in these conditions.
1.b) modify the weight of some classes (set to 10 the weights for misclassification between virginica into versicolor and vice versa) and learn the tree in these conditions. You should obtain similar results as for step 1.
2. learn trees but try to avoid overfitting (by improving the error on the test set) tuning the hyper-parameters on: the minimum number of samples per leaf, max depth of the tree, min_impurity_decrease parameters, max leaf nodes, etc.
3. build the confusion matrix of the created tree models on the test set and show them.
4. build the ROC curves (or coverage curves in coverage space) and plot them for each tree model you have created: for each model you have to build three curves, one for each class, considered in turn as the positive class.
"""
data_boosted = np.tile(iris.data[(iris.target == 1) | (iris.target == 2)], (10, 1))
data_boosted = np.vstack([data_boosted, iris.data[(iris.target == 0)]])
target_boosted = np.tile(iris.target[(iris.target == 1) | (iris.target == 2)], 10)
target_boosted = np.hstack([target_boosted, iris.target[(iris.target == 0)]])

indeces_boosted = np.random.permutation(len(data_boosted))
indeces_training_boosted = indeces_boosted[:-100]
indices_test_boosted = indeces_boosted[-100:]

iris_X_train_boosted = data_boosted[indeces_training_boosted]
iris_y_train_boosted = target_boosted[indeces_training_boosted]
iris_X_test_boosted = data_boosted[indices_test_boosted]
iris_y_test_boosted = target_boosted[indices_test_boosted]

clf_boosted = tree.DecisionTreeClassifier(criterion="entropy",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})
clf_boosted = clf_boosted.fit(iris_X_train_boosted, iris_y_train_boosted)

predicted_y_test_boosted = clf_boosted.predict(iris_X_test_boosted)

acc_score = accuracy_score(iris_y_test_boosted, predicted_y_test_boosted)
print("Accuracy score: "+ str(acc_score))
f1=f1_score(iris_y_test_boosted, predicted_y_test_boosted, average='macro')
print("F1 score: "+str(f1))
